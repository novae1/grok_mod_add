{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None, xaxis=None, yaxis=None, x_points=None, y_points=None):\n",
    "    px.imshow(\n",
    "        utils.to_numpy(image),\n",
    "        title=title,\n",
    "        labels={'x': xaxis, 'y': yaxis},\n",
    "        x=x_points,\n",
    "        y=y_points,\n",
    "        color_continuous_midpoint=0.,\n",
    "        color_continuous_scale='RdBu',\n",
    "        aspect='auto'\n",
    "    ).show()\n",
    "\n",
    "def line(tensor, renderer=None, xaxis='', yaxis='', **kwargs):\n",
    "    px.line(\n",
    "        utils.to_numpy(tensor),\n",
    "        labels={\"x\":xaxis, \"y\":yaxis},\n",
    "        **kwargs\n",
    "    ).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis='', yaxis='', caxis='', renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(\n",
    "        y=y, x=x,\n",
    "        labels={'x':xaxis, 'y':yaxis, 'color':caxis},\n",
    "        **kwargs\n",
    "    ).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_heatmap(z):\n",
    "    heatmap = go.Heatmap(\n",
    "        z=z,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0.,\n",
    "        showscale=False,\n",
    "    )\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the model's saved\n",
    "PTH_LOCATION = 'modadd.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 113\n",
    "frac_train = 0.3\n",
    "\n",
    "# Optimizer config\n",
    "lr = 1e-3\n",
    "wd = 1.\n",
    "betas = (0.9, 0.98)\n",
    "\n",
    "num_epochs = 25000\n",
    "checkpoint_every = 100\n",
    "\n",
    "DATA_SEED = 598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input format is |a|b|=|\n",
    "a_vector = einops.repeat(torch.arange(p), 'i -> (i j)', j=p)\n",
    "b_vector = einops.repeat(torch.arange(p), 'j -> (i j)', i=p)\n",
    "equals_vector = einops.repeat(torch.tensor(113), ' -> (i j)', i=p, j=p)\n",
    "\n",
    "print(a_vector, b_vector, equals_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1)\n",
    "print(dataset[:5])\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (dataset[:, 0] + dataset[:, 1]) % p\n",
    "print(labels[:5])\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset into train + test set, 30% in training set\n",
    "torch.manual_seed(DATA_SEED)\n",
    "indices = torch.randperm(p*p)\n",
    "cutoff = int(p*p*frac_train)\n",
    "train_indices = indices[:cutoff]\n",
    "test_indices = indices[cutoff:]\n",
    "\n",
    "train_data = dataset[train_indices]\n",
    "train_labels = labels[train_indices]\n",
    "test_data = dataset[test_indices]\n",
    "test_labels = labels[test_indices]\n",
    "\n",
    "print(train_data[:5])\n",
    "print(train_labels[:5])\n",
    "print(train_data.shape)\n",
    "\n",
    "print(test_data[:5])\n",
    "print(test_labels[:5])\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers=1,\n",
    "    n_heads=4,\n",
    "    d_model=128,\n",
    "    d_head=32,\n",
    "    d_mlp=512,\n",
    "    act_fn='relu',\n",
    "    normalization_type=None,\n",
    "    d_vocab=p+1,\n",
    "    d_vocab_out=p,\n",
    "    n_ctx=3,\n",
    "    init_weights=True,\n",
    "    device='cpu',\n",
    "    seed=999,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable biases because they aren't needed and make model harder to interpret\n",
    "for name, param in model.named_parameters():\n",
    "    if 'b_' in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer + Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    if len(logits.shape) == 3:\n",
    "        logits = logits[:, -1]\n",
    "    logits = logits.to(torch.float64)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
    "    return -correct_log_probs.mean()\n",
    "\n",
    "train_logits = model(train_data)\n",
    "train_loss = loss_fn(train_logits, train_labels)\n",
    "print(train_loss)\n",
    "test_logits = model(test_data)\n",
    "test_loss = loss_fn(test_logits, test_labels)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what loss would be if guess was uniformly random\n",
    "print(\"Uniform loss:\")\n",
    "print(np.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll train the model with the whole batch at once, not SGD\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "model_checkpoints = []\n",
    "checkpoint_epochs = []\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        train_logits = model(train_data)\n",
    "        train_loss = loss_fn(train_logits, train_labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (epoch + 1) % checkpoint_every == 0:\n",
    "            train_losses.append(train_loss.item())\n",
    "            with torch.inference_mode():\n",
    "                test_logits = model(test_data)\n",
    "                test_loss = loss_fn(test_logits, test_labels)\n",
    "                test_losses.append(test_loss.item())\n",
    "\n",
    "            checkpoint_epochs.append(epoch)\n",
    "            model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
    "            print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    torch.save(\n",
    "        {\n",
    "            'model': model.state_dict(),\n",
    "            'config': model.cfg,\n",
    "            'checkpoints': model_checkpoints,\n",
    "            'checkpoint_epochs': checkpoint_epochs,\n",
    "            'test_losses': test_losses,\n",
    "            'train_losses': train_losses,\n",
    "            'train_indices': train_indices,\n",
    "            'test_indices': test_indices\n",
    "        },\n",
    "        PTH_LOCATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    cached_data = torch.load(PTH_LOCATION)\n",
    "    model.load_state_dict(cached_data['model'])\n",
    "    model_checkpoints = cached_data['checkpoints']\n",
    "    checkpoint_epochs = cached_data['checkpoint_epochs']\n",
    "    test_losses = cached_data['test_losses']\n",
    "    train_losses = cached_data['train_losses']\n",
    "    train_indices = cached_data[\"train_indices\"]\n",
    "    test_indices = cached_data[\"test_indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_trace = go.Scatter(\n",
    "    x=np.arange(len(train_losses)),\n",
    "    y=train_losses,\n",
    "    mode='lines', name='Train Loss'\n",
    ")\n",
    "test_loss_trace = go.Scatter(\n",
    "    x=np.arange(len(test_losses)),\n",
    "    y=test_losses,\n",
    "    mode='lines', name='Test Loss'\n",
    ")\n",
    "fig = go.Figure()\n",
    "fig.add_trace(train_loss_trace)\n",
    "fig.add_trace(test_loss_trace)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model(train_data), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logits for all inputs and cache\n",
    "original_logits, cache = model.run_with_cache(dataset)\n",
    "params = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll often be confused about what each hook is. To test, you can always print the hooks, multiply them by some parameters and compare them with other hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing all cache items\n",
    "for name, param in cache.items():\n",
    "    print(name, param.shape, sep=': ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing model layers & parameters\n",
    "for param_name in params.keys():\n",
    "    print(param_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the embedding matrix W_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = params['embed.W_E'][:-1].detach()\n",
    "imshow(W_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is confusing. We can perform SVD on this matrix to get information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = torch.svd(W_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(U)\n",
    "imshow(U * S)\n",
    "line(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a few vectors here really matter. I think if we limit ourselves up to (and including) row 11 we can get most of it. To check if this is indeed the case, we can compare the 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ = U.clone()\n",
    "U_[:, 12:] = 0.\n",
    "# Check if there's any difference between U and only U's 15 first rows\n",
    "((U-U_) @ torch.diag(S) @ V.T).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some information loss, but not too much. The biggest difference is one order of magnitude smaller than the largest values of W_E. There might be an impact, but for now let's analyze just the first 12 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(U[:, :12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the Fast Fourier Transform to find the waves represented by each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_freq = np.fft.fft(U[:, :12], axis=0)\n",
    "\n",
    "imshow(np.real(U_freq), title=\"real freq\", xaxis='column')\n",
    "imshow(np.imag(U_freq), title='imag freq', xaxis='column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got GPT-4 to write some code that finds the wave that fits the data. I don't really understand what it's doing, but it works great!\n",
    "\n",
    "I should really learn how the FFT, discrete and normal Fourier transforms work. They seem super useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_plot(data, num=1000):\n",
    "    # Perform the Fast Fourier Transform (FFT) on the input data\n",
    "    fft_data = np.fft.fft(data)\n",
    "\n",
    "    # Calculate the amplitudes (absolute values) of the FFT result\n",
    "    amplitudes = np.abs(fft_data)\n",
    "\n",
    "    # Find the index of the dominant frequency\n",
    "    dominant_frequency_index = np.argmax(amplitudes[1:]) + 1\n",
    "    if dominant_frequency_index > len(data) / 2:  # Handle aliasing for real-valued input signal\n",
    "        dominant_frequency_index = len(data) - dominant_frequency_index\n",
    "\n",
    "    # Calculate the dominant frequency in cycles per data point and its amplitude\n",
    "    dominant_frequency = dominant_frequency_index / len(data)\n",
    "    dominant_amplitude = amplitudes[dominant_frequency_index] / (len(data) / 2)  # Correct for symmetric FFT of real-valued signal\n",
    "\n",
    "    # Calculate the phase shift of the dominant frequency component\n",
    "    phase_shift = np.angle(fft_data[dominant_frequency_index])\n",
    "\n",
    "    # Create sequences of evenly spaced values for the original and the reconstructed data\n",
    "    x_values_data = np.linspace(0, len(data) - 1, len(data))\n",
    "    x_values_reconstructed = np.linspace(0, len(data) - 1, num)  # Use num to control the resolution of the reconstructed function\n",
    "\n",
    "    # Reconstruct the function with the dominant frequency\n",
    "    reconstructed_function = dominant_amplitude * np.cos(2 * np.pi * dominant_frequency * x_values_reconstructed + phase_shift)\n",
    "\n",
    "    # Create an empty plotly graph object\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add the original data and the reconstructed function to the plot\n",
    "    fig.add_trace(go.Scatter(x=x_values_data, y=data, mode='lines', name='Original data'))\n",
    "    fig.add_trace(go.Scatter(x=x_values_reconstructed, y=reconstructed_function, mode='lines', name='Reconstructed function'))\n",
    "\n",
    "    # Create dictionary with data from function\n",
    "\n",
    "    return fig, f\"f(x) = {dominant_amplitude} * cos(2 * pi * {dominant_frequency} * x + {phase_shift})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=6)\n",
    "for col in range(6):\n",
    "    for row in range(2):\n",
    "        # Suppose that the analyze_and_plot function returns a figure with a single trace\n",
    "        figure = analyze_and_plot(U[:, 2 * col + row])[0]\n",
    "        for trace in figure['data']:\n",
    "            fig.add_trace(\n",
    "                trace,\n",
    "                row=row + 1, col=col + 1\n",
    "            )\n",
    "fig.update_layout(height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pairs (0-1 up to 10-11) represent pairs of sine-cosine, where one wave is shifted +pi/2 to the right (or left). I'll probably have to save the frequencies, amplitudes and phase-shifts, because I believe I'll need them for analysis later.\n",
    "\n",
    "Nonetheless, the analysis of W_E is basically done. Let's move on to other layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the self-attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neel had a super good idea: plot the attention patterns in a 113 x 113 gird. Keep an eye out for cool visualizations like this, and always try to find innovatives ways to visualize data -- that can save you a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_pattern = cache['pattern', 0]\n",
    "print(attn_pattern.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange attn pattern into a 113 x 113 grid\n",
    "total_square_attn = einops.rearrange(attn_pattern, '(i j) h a b -> h i j a b', i=p, j=p)\n",
    "print(total_square_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only care about the attn of Q: '=', K: 'a' and 'b'\n",
    "square_attn = total_square_attn[:, :, :, -1, :-1]\n",
    "print(square_attn.shape)\n",
    "\n",
    "fig_square_attn = make_subplots(\n",
    "    # 'a' in row 1, 'b' in row 2, cols correspond to attn heads\n",
    "    rows=2, cols=4,\n",
    "    subplot_titles=[\n",
    "        f\"Head {col} Input {'a' if row%2==0 else 'b'}\"\n",
    "        for row in range(2) for col in range(4)\n",
    "    ],\n",
    "    vertical_spacing=0.1,\n",
    "    horizontal_spacing=0.05\n",
    ")\n",
    "\n",
    "for row in range(2):\n",
    "    for col in range(4):\n",
    "        fig_square_attn.add_trace(\n",
    "            std_heatmap(square_attn[col, :, :, row]),\n",
    "            row=row+1, col=col+1,\n",
    "        )\n",
    "fig_square_attn.update_layout(height=800)\n",
    "fig_square_attn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is suuuuuuuper periodic. Let's take the 2-D FFT to check if and how this can be reduced into waves, then ask GPT-4 to help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 2-D FFT over the axis of length 113 (1 and 2)\n",
    "fft_square_attn = np.fft.fft2(square_attn, axes=(1, 2))\n",
    "print(fft_square_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fft_square_attn = make_subplots(\n",
    "    # One row for each (head-input) pair\n",
    "    # Cols are graph--real--imag, graph is the attn patterns, real + imag are from the FFT\n",
    "    rows=8, cols=3,\n",
    "    subplot_titles=[\n",
    "        f\"Head {row//2} Input {'a' if row%2==0 else 'b'} {'graph' if col%3==0 else ('real' if col%3==1 else 'imag')}\"\n",
    "        for row in range(8) for col in range(3)\n",
    "    ],\n",
    "    vertical_spacing=0.02,\n",
    "    horizontal_spacing=0.05\n",
    ")\n",
    "\n",
    "for row in range(8):\n",
    "    for col in range(3):\n",
    "        # Show either graph, real of imag part of FFT\n",
    "        head = row // 2\n",
    "        a_or_b = row % 2\n",
    "        # Shift the FFT graph to the center\n",
    "        centered_fft = np.fft.fftshift(fft_square_attn[head, :, :, a_or_b])\n",
    "\n",
    "        match col:\n",
    "            case 0:\n",
    "                # Show graph\n",
    "                z = square_attn[head, :, :, a_or_b]\n",
    "            case 1:\n",
    "                # Show centered real part of FFT\n",
    "                z = np.real(centered_fft)\n",
    "            case 2:\n",
    "                # Show centered imag part of FFT\n",
    "                z = np.imag(centered_fft)\n",
    "        \n",
    "        fig_fft_square_attn.add_trace(\n",
    "            std_heatmap(z),\n",
    "            row=row+1, col=col+1,\n",
    "        )\n",
    "fig_fft_square_attn.update_layout(height=3200)\n",
    "fig_fft_square_attn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairs 0-1 and 2-3 look very similar. I'm trying to understand what they're doing. They're representing waves, and depending on the inputs it'll give more importance to 'a' or 'b'. How does this work, and what does this mean? \n",
    "\n",
    "1. One graph is the transpose of the other. The following example will make this clear: Our inputs are (a, b). Attention is calculated irrespective of the other values, and since 'n' in pos 'a' is (practically) the same as 'n' in pos 'b' (I did this analysis somewhere), picking (b, a) will reverse the attention.\n",
    "\n",
    "2. (3, 1) has attention (0.2, 0.8). Why is this? Why are we getting 0.2 of the Value of 3 and 0.8 of the Value of 1? What does this mean, and what is happening here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only a few frequencies matter. Let's create a function to reduce the attention heads' patterns to the bare minimum (only the most important frequencies), and then check if they do indeed approximate the real attention patterns well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_matrix_fft(matrix, n_top=1):\n",
    "    '''\n",
    "    In: wavy real square matrix\n",
    "    Out: matrix reduced to its n_top greatest frequencies on the main axes\n",
    "    Maybe I'll also make it return the waves' data (not for now)\n",
    "    '''\n",
    "    fft_matrix = np.fft.fft2(matrix)\n",
    "    \n",
    "    # Sorted (ascending order) list of indexes of greatest absolute value\n",
    "    sort_id_rows = np.abs(fft_matrix[:, 0]).argsort()\n",
    "    sort_id_cols = np.abs(fft_matrix[0, :]).argsort()\n",
    "\n",
    "    # Put 1 + 2*n_top greatest values of each row & col in a zeros vector\n",
    "    reduced_fft_matrix = np.zeros_like(matrix, dtype=np.complex128)\n",
    "    reduced_fft_matrix[:, 0][sort_id_rows[-2*n_top-1:]] = fft_matrix[:, 0][sort_id_rows[-2*n_top-1:]]\n",
    "    reduced_fft_matrix[0, :][sort_id_cols[-2*n_top-1:]] = fft_matrix[0, :][sort_id_cols[-2*n_top-1:]]\n",
    "\n",
    "    # Apply inverse FFT\n",
    "    reduced_matrix = np.real(np.fft.ifft2(reduced_fft_matrix))\n",
    "\n",
    "    return torch.from_numpy(reduced_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if approximation is good, and the effect of 'n_top' on the approximation\n",
    "for head in range(4):\n",
    "    errors = []\n",
    "    for n_top in range(114):\n",
    "    # Take attention pattern of head 'head', 'a' if a_or_b = 0, 'b' if = 1\n",
    "        head_attn = square_attn[head, :, :, 0]\n",
    "        error = torch.abs(reduce_matrix_fft(head_attn, n_top=n_top) - head_attn).mean()\n",
    "        errors.append(100 * error / head_attn.mean())\n",
    "    line(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MORE IDEAS TO TRY**: \n",
    "\n",
    "1. Look at the relation between equivalent sums in the attention heads and after the QK - WV circuit. Hopefully equivalent sums will have equivalent vectors there, which will make things easier to interpret. Be sure to look *before* the residual stream.\n",
    "\n",
    "2. Find similar relations in the attention patterns themselves? Maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the circuit from W_E to the attention patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 'a' and 'b', what matters is what's at the end of **W_E -> W_pos -> W_K**\n",
    "\n",
    "For '=', what matters is what's at the end of **W_E -> W_pos -> W_Q**\n",
    "\n",
    "Our preliminary analysis was very important to understand what's going on, and I believe that the tools we've used before will be of use here too. Now I'll try to analyze what comes right before the attention is calculated. For starters, I want to plot all inputs in **pos 0** (a), all inputs in **pos 1** (b), and the final form of '=' after passing through all the intermediate layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make the way to our destination by hand, so that I get some practice with batch multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the dataset\n",
    "dataset_one_hot = F.one_hot(dataset).float() # The dtype has to be float\n",
    "print(dataset_one_hot.shape, params['embed.W_E'].shape)\n",
    "\n",
    "# Pass dataset through embedding matrix\n",
    "dataset_embed = einops.einsum(dataset_one_hot, params['embed.W_E'], 'batch pos d_in, d_in d_model -> batch pos d_model')\n",
    "\n",
    "# Check if it's the same as hook after W_E\n",
    "print((dataset_embed == cache['embed']).float().mean())\n",
    "\n",
    "# Add positional embedding\n",
    "dataset_pos = dataset_embed + params['pos_embed.W_pos']\n",
    "\n",
    "# Check if it's the same as the residual stream before attn\n",
    "print((dataset_pos == cache['resid_pre', 0]).float().mean())\n",
    "\n",
    "# Get Keys and Queries\n",
    "dataset_K = einops.einsum(dataset_pos, params['blocks.0.attn.W_K'], 'batch pos d_model, head d_model d_head -> batch pos head d_head')\n",
    "dataset_Q = einops.einsum(dataset_pos, params['blocks.0.attn.W_Q'], 'batch pos d_model, head d_model d_head -> batch pos head d_head')\n",
    "\n",
    "# Check if it's the same as hook_k and hook_q\n",
    "print((dataset_K == cache['k', 0]).float().mean())\n",
    "print((dataset_Q == cache['q', 0]).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to display the things we're interested in: Queries of '=' for each head keys for 'a' and 'b' for all numbers 0-112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pick just the first element of the batch ([0, 0, 113]) because '=' is the same in all of them\n",
    "q_equals = cache['q', 0][1, -1, :, :]\n",
    "\n",
    "imshow(q_equals, yaxis='heads', title=\"Query of '=' for each head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print difference between keys of 'a' and 'b'\n",
    "for head in range(4):\n",
    "    k = cache['k', 0]\n",
    "    # Get keys of 'a' and 'b'\n",
    "    k_a = k[::113, 0, head, :]\n",
    "    k_b = k[:113, 1, head, :]\n",
    "\n",
    "    # Compute maximum absolute difference\n",
    "    error = np.abs(k_a - k_b).max()\n",
    "\n",
    "    # Print percentage error in relation to k_a's mean\n",
    "    print(100 * error / np.abs(k_a).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the difference between the Keys of 'a' and 'b' is never greater than 1% (which was expected, since addition is commutative). Thus, we may restrict our analysis to position 'a', since 'b''s analysis will be practically identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_keys_graph_and_fft = make_subplots(\n",
    "    # One row for each (head-input) pair\n",
    "    # Cols are graph--real--imag, graph is the Key of 'a', real + imag are from the FFT\n",
    "    rows=4, cols=3,\n",
    "    subplot_titles=[\n",
    "        f\"Head {head}, 'a' {'graph' if col%3==0 else ('real' if col%3==1 else 'imag')}\"\n",
    "        for head in range(4) for col in range(3)\n",
    "    ],\n",
    "    vertical_spacing=0.05,\n",
    "    horizontal_spacing=0.05,\n",
    ")\n",
    "\n",
    "for head in range(4):\n",
    "    for col in range(3):\n",
    "        # Show either graph of Keys of 'a', real of imag part of FFT\n",
    "        graph = cache['k', 0][::113, 0, head, :]\n",
    "\n",
    "        match col:\n",
    "            case 0:\n",
    "                # Show graph\n",
    "                z = graph\n",
    "            case 1:\n",
    "                # Show centered real part of FFT\n",
    "                z = np.real(np.fft.fftshift(np.fft.fft(graph, axis=0)))\n",
    "            case 2:\n",
    "                # Show centered imag part of FFT\n",
    "                z = np.imag(np.fft.fftshift(np.fft.fft(graph, axis=0)))\n",
    "        \n",
    "        fig_keys_graph_and_fft.add_trace(\n",
    "            std_heatmap(z),\n",
    "            row=head+1, col=col+1\n",
    "        )\n",
    "fig_keys_graph_and_fft.update_layout(height=1600)\n",
    "fig_keys_graph_and_fft.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT IDEAS:\n",
    "\n",
    "1. The graphs above are **extremely** periodic, and the frequencies are the same throughout all the head dimensions. This means that, **very likely**, all of the 113 arrays are just one specific array multiplied by a scalar (which is a function of many frequencies). Since what we do is take the dot product between this and '=''s Query, the resulting dot product will be a constant (fixed for all 113) multiplied by that scalar determined by the frequencies.\n",
    "\n",
    "2. Maybe one reason why the attention patterns weren't as periodic as the graphs above was because of the softmax. To check if I'm right, perform FFT on the attention pattern *before* softmax.\n",
    "\n",
    "3. You might be asking yourself, future Nicolas, why I didn't perform the 2-D FFT, since it's **obviously** periodic in both dimensions. I asked myself the same question, and apparently the FFT disagrees.\n",
    "\n",
    "We can check both of these hypothesis by performing the dot product between Queries and Keys, and then applying the FFT again. If the frequencies are similar, than (I believe) I'll be proven correct. Either way, what we *really* need to analyze is that dot product, so let's get started rn! Since we've shown elsewhere that 'a' and 'b' are basically identical (any difference between them is *at least* smaller than 10^-3), we'll restric our analysis to position 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Query of '=', Key of 'a'\n",
    "query_equals = cache['q', 0][0, -1, :, :] # again, we can pick the first one because all are equal\n",
    "keys_a = cache['k', 0][::113, 0, :, :]\n",
    "\n",
    "print(\"keys_a:\", keys_a.shape)\n",
    "print(\"query_equals:\", query_equals.shape)\n",
    "\n",
    "# Calculate the unnormalized attention of 'a'\n",
    "attn_a = einops.einsum(keys_a, query_equals, 'num head d_head, head d_head -> num head')\n",
    "\n",
    "# Show absolute value of FFT of unnormalized attention of 'a'\n",
    "imshow(np.abs(np.fft.fft(attn_a, axis=0)), xaxis='heads', yaxis='nums')\n",
    "\n",
    "\n",
    "# We want to check if this has the same frequencies as 'a''s keys\n",
    "# Concatenate different heads into 1 dimension so that it has same shape as unnorm attn\n",
    "concat_keys_a = einops.rearrange(keys_a, 'nums head d_head -> nums (head d_head)')\n",
    "\n",
    "# Show absolute value of FFT of 'a''s Keys, check if they look the same\n",
    "imshow(np.abs(np.fft.fft(concat_keys_a, axis=0)), xaxis='heads', yaxis='nums')\n",
    "\n",
    "imshow(attn_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I WAS CORRECT FUCK YEAH\n",
    "\n",
    "This means now the only things I have to analyze are:\n",
    "\n",
    "1. The attention pattern before softmax\n",
    "2. How different numbers add to '=''s residual stream\n",
    "3. What the MLP does\n",
    "\n",
    "**IDEAS**: \n",
    "\n",
    "1. You haven't looked at the Values matrices at all, they might contain some useful insight\n",
    "\n",
    "2. PLOT 'attn_a' AS LINES AND DO CURVE-FITTING YOU MORON\n",
    "\n",
    "**HYPOTHESIS**: Look at how clean these lines are. Super fucking clean if I may say so. Maybe what softmax is doing is 'corrupt' the pretty lines, that's why we don't get frequencies as nice as the ones here after the softmax. I believe this is true. Actually no, look below. One of the problems is with the Values, they don't have frequencies as clean as these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attn_a and \n",
    "fig_attn_a = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    subplot_titles=[f'head {head}' for head in range(4)],\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "for head in range(4):\n",
    "    figure, data = analyze_and_plot(attn_a[:, head], num=113)\n",
    "    print(data)\n",
    "    for trace in figure['data']:\n",
    "        fig_attn_a.add_trace(\n",
    "            trace,\n",
    "            row=head + 1, col=1\n",
    "        )\n",
    "fig_attn_a.update_layout(height=800)\n",
    "fig_attn_a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uhhh, this is weird. WHY DOES JUST ONE FREQUENCY FIT THE WAVE SO WELL?? I'll get GPT-4 to write code that gets the 4 main frequencies (when I get it back ;-;), but this is already surprisingly accurate. What if we try to understand what's going on by using just these simplifications? Maybe we'll actually get pretty far, and we won't need GPT-4's help.\n",
    "\n",
    "**IDEA**: With the cosine wave on hand, calculate and plot the attention graph from the pure waves and compare it to the original. If it's close enough, celebrate! You can also go through the whole process and get the outputs based on these. If they're decently high, you'll be basically done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The indexes are for the heads 0-4. Maybe I should cut some digits\n",
    "head_cos_wave = {\n",
    "    'dominant_amplitude': [6.329031410795562, 8.48166410569167, 6.196583114665829, 7.722471474643398],\n",
    "    'dominant_frequency': [0.36283185840707965, 0.36283185840707965, 0.07079646017699115, 0.07079646017699115],\n",
    "    'phase_shift': [2.3973650351858686, -0.8454673633717475, 2.9620057309991714, -0.1647056964663721],\n",
    "}\n",
    "\n",
    "# Waves for unnormalized attention of 'a' and 'b'\n",
    "x_0_to_112 = torch.linspace(start=0, end=112, steps=113)\n",
    "cos_x = torch.stack([\n",
    "    (head_cos_wave['dominant_amplitude'][head] * torch.cos(2*torch.pi*x_0_to_112*head_cos_wave['dominant_frequency'][head] + head_cos_wave['phase_shift'][head]))\n",
    "    for head in range(4)]\n",
    ")\n",
    "\n",
    "# Display waves\n",
    "line(einops.rearrange(cos_x, 'a b -> b a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct attention scores from the waves\n",
    "reconstructed_attn_scores = torch.tensor([[[\n",
    "    [cos_x[head][a] / math.sqrt(32), cos_x[head][b] / math.sqrt(32)] # Divide by d_head for normalization\n",
    "    for b in range(113)]\n",
    "    for a in range(113)]\n",
    "    for head in range(4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate attention pattern\n",
    "reconstructed_attn_pattern = reconstructed_attn_scores.softmax(-1)\n",
    "\n",
    "# Compare reconstructed and original attn patterns\n",
    "imshow(reconstructed_attn_pattern[3, :, :, 1])\n",
    "imshow(square_attn[3, :, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks SOOO good. They're so similar!!! I'll pass it through the rest of the neural net to see if I get a good result. If so, I'm basically done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange it into the original (reduced) shape\n",
    "reshaped_reconstructed_attn_pattern = einops.rearrange(reconstructed_attn_pattern, 'head a b a_b -> (a b) a_b head 1')\n",
    "\n",
    "# Get values for a and b (12769, 2, 4, 32)\n",
    "v_a_and_b = cache['v', 0][:, :-1]\n",
    "\n",
    "# Get z (12769, 4, 32)\n",
    "reconstructed_z = (reshaped_reconstructed_attn_pattern * v_a_and_b).sum(1)\n",
    "print(reconstructed_z.shape)\n",
    "\n",
    "# Get attn_out (12769, d_model)\n",
    "print(params['blocks.0.attn.W_O'].shape)\n",
    "reconstructed_attn_out = einops.einsum(reconstructed_z, params['blocks.0.attn.W_O'], 'b h d_head, h d_head d_model -> b d_model')\n",
    "\n",
    "# Get resid_mid (12796, 128)\n",
    "reconstructed_resid_mid = cache['resid_pre', 0][:, -1] + reconstructed_attn_out\n",
    "\n",
    "# Get logits\n",
    "reconstructed_logits = pass_mlp(reconstructed_resid_mid)\n",
    "\n",
    "# Get probs\n",
    "reconstructed_probs = reconstructed_logits.softmax(-1)\n",
    "\n",
    "# Store all vectors in equivalence classes (same class if same sum)\n",
    "sorted_probs = [[] for _ in range(p)]\n",
    "\n",
    "for a in range(p):\n",
    "    for b in range(p):\n",
    "        sum = (a + b) % p\n",
    "        index = a * p + b\n",
    "        # Put vector of sum 'sum' in index class 'sum'\n",
    "        sorted_probs[sum].append(utils.to_numpy(reconstructed_probs[index]))\n",
    "\n",
    "# Transform into ndarray with shape (113, 113, 128) -> (class/sum, num, d_model)\n",
    "# 'num' is just a number from 0 to 112, not necessarily in order because the for loop above doesn't track that\n",
    "sorted_probs = np.array(sorted_probs)\n",
    "\n",
    "# Stack them for improved visualization\n",
    "stack_probs = einops.rearrange(sorted_probs, 'sum num d_model -> (sum num) d_model')\n",
    "\n",
    "imshow(stack_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IT'S FUCKING DONE AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the model after the attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STOP DISPLAYING HUGE GRAPHS, IT LAGS THE FUCK OUT OF VS CODE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDEA I ONLY HAD TOO LATE**: Look at the residual stream for '=' after the attention layer: I expect equivalent sums (0 + 3 and 2 + 1) to have very similar vectors. If they dont, then it'll definitely be the job of the MLP to make sure they're similar. Since the loss is very small, *somewhere* after the attention equivalent sums **must** have very similar '='s vectors.\n",
    "\n",
    "Let's start by looking at the residual stream after the MLP. There the model should already have 'sorted' equivalent sums in equivalent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get residual stream of '=' after MLP\n",
    "resid_post = cache['resid_post', 0][:, -1]\n",
    "\n",
    "# Store all vectors in equivalence classes (same class if same sum)\n",
    "ordered_resid_post = [[] for _ in range(p)]\n",
    "\n",
    "for a in range(p):\n",
    "    for b in range(p):\n",
    "        sum = (a + b) % p\n",
    "        index = a * p + b\n",
    "        # Put vector of sum 'sum' in index class 'sum'\n",
    "        ordered_resid_post[sum].append(utils.to_numpy(resid_post[index]))\n",
    "\n",
    "# Transform into ndarray with shape (113, 113, 128) -> (class/sum, num, d_model)\n",
    "# 'num' is just a number from 0 to 112, not necessarily in order because the for loop above doesn't track that\n",
    "ordered_resid_post = np.array(ordered_resid_post)\n",
    "\n",
    "# Stack them for improved visualization\n",
    "stack_orp = einops.rearrange(ordered_resid_post, 'sum num d_model -> (sum num) d_model')\n",
    "imshow(stack_orp[24*128 : 27*128], title=\"Stacked classes 24-26 of resid_post\") # Only look at a few classes because visualizing everything lags my pc\n",
    "\n",
    "# Also concatenate them because I'll need it for later\n",
    "cat_orp = einops.rearrange(ordered_resid_post, 'sum num d_model -> num (sum d_model)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what I wanted to see: vectors which represent the same sum are, *indeed*, similar. Unfortunately, this doesn't seem to be the case for the vectors before the MLP (look below), which will be a pain in the ass. Hopefully they have a cool representation.\n",
    "\n",
    "Below we're doing the same thing as above, but with the residual stream before the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get residual stream of '=' before MLP\n",
    "resid_mid = cache['resid_mid', 0][:, -1]\n",
    "\n",
    "# Store all p * p vectors in equivalence classes (same class if same sum)\n",
    "ordered_resid_mid = [[] for _ in range(p)]\n",
    "\n",
    "for a in range(p):\n",
    "    for b in range(p):\n",
    "        sum = (a + b) % p\n",
    "        index = a * p + b\n",
    "        # Put vector of sum 'sum' in index class 'sum'\n",
    "        ordered_resid_mid[sum].append(utils.to_numpy(resid_mid[index]))\n",
    "\n",
    "# Transform into ndarray with shape (113, 113, 128) -> (class/sum, num, d_model)\n",
    "# 'num' is just a number from 0 to 112, not necessarily in order because the for loop above doesn't track that\n",
    "ordered_resid_mid = np.array(ordered_resid_mid)\n",
    "\n",
    "# Concatenate them for (hopefully) better visualization\n",
    "# Here concatenating is better because there's no apparent relation between vectors of the same class\n",
    "cat_orm = einops.rearrange(ordered_resid_mid, 'sum num d_model -> num (sum d_model)')\n",
    "imshow(cat_orm[:, 24*128 : 27*128], title=\"Concatenated classes 24-26 of resid_mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a random mess AAAAAAAAAAAAAAAAAAA. What we wanted was to see homogenous columns, which would imply that vectors from the same class are similar. Well, tbh it's not *that* bad, you can see the wavy patterns pretty well, FFT will do a good job with this.\n",
    "\n",
    "**IDEA**: Maybe what the MLP does is extract a certain subspace from the vectors. If so, FFT should make 'resid_mid' interpretable. Hopefully. Tonight, I'll pray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform FFT on concatenated Ordered Residual Mid to hopefully extract some info\n",
    "fft_cat_orm = np.fft.fft(cat_orm[:, 24*128 : 27*128], axis=0) # Only doing with these sections because lag\n",
    "\n",
    "# Display abs of FFT of concatenated ORM\n",
    "imshow(np.abs(fft_cat_orm), title=\"FFT of classes 24-26 of resid_mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF, I didn't expect this at all. Why can they all be represented with the same frequencies? This is extremely surprising.\n",
    "\n",
    "**IMPORTANT REALIZATION** *A priori*, there's no reason for them to be ordered in any way, because the order in which I picked members from each equivalence class wasn't orderly. It wasn't random either, but I presume (until I think better of it) that the ordering shouldn't matter. Therefore, I should try to find another way to extract information from vectors in the same equivalence class that doesn't use the FFT (at least not at first). Some possible ideas are:\n",
    "\n",
    "1. Pass it through the first layer of the MLP to see if the data gets more orderly, and how that happens.\n",
    "2. Other ideas\n",
    "\n",
    "**To recapitulate**: each 128-long block in the x axis represents one class. For some reason, all classes can be represented using the same frequencies (or at least it seems so). This is very surprising and unexpected, because *a priori* there's no reason for different equivalence classes to be represented by the same frequencies (or to have a similar wave-structure). Think of why this could be the case:\n",
    "\n",
    "1. The attention heads represent stuff as waves so something something all waves are the same what changes is where they are something something.\n",
    "\n",
    "Let's check how many frequencies we need to make an omelete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get abs max of each row of fft_cat_orm\n",
    "max_fft_cat_orm = np.abs(fft_cat_orm).max(axis=1)\n",
    "\n",
    "for i in range(p //2): # Only the lower-half matters, the other part is reflected\n",
    "    if max_fft_cat_orm[i] > 10: # The biggest below 10 is less than 7, can be ignored\n",
    "        print(i, max_fft_cat_orm[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 different frequencies. This feels tractable. It'd be cool if we managed to reverse-engineer what the MLP is doing by finding the differences in frequencies.\n",
    "\n",
    "Speaking of which, I haven't used the FFT on 'resid_post'. Maybe it'll give some insight. However, I need to sleep now. FUCK SLEEPING WE MUST DO SCIENCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take FTT of 'resid_post' wrt columns (which look very similar already)\n",
    "fft_cat_orp = np.fft.fft(cat_orp, axis=0)\n",
    "\n",
    "# Get abs max of each row of fft_cat_orp\n",
    "max_fft_cat_orp = np.abs(fft_cat_orp).max(axis=1)\n",
    "\n",
    "# Check if there are any big ones\n",
    "for i in range(p): # Only the lower-half matters, the other part is reflected\n",
    "    if max_fft_cat_orp[i] > 100:\n",
    "        print(i, max_fft_cat_orp[i])\n",
    "\n",
    "imshow(np.abs(fft_cat_orp[:, 24*128 : 27*128]), title=\"FFT of classes 24-26 of resid_post\") # My computer can't handle this shit anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very good news, they can be explained with basically just one frequency (all others are at least one order of magnitude smaller). Let's try reducing the \"Ordered Residual Post\" to just its highest frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce fft_cat_orp to one its greatest frequency\n",
    "red_fft_cat_orp = np.zeros_like(fft_cat_orp, dtype=np.complex128)\n",
    "red_fft_cat_orp[0] = fft_cat_orp[0]\n",
    "\n",
    "# Perform the inverse fourier transform to get reduced cat_orp\n",
    "red_cat_orp = np.real(np.fft.ifft(red_fft_cat_orp, axis=0)) # The imaginary part will be negligeble because the original was real\n",
    "\n",
    "# Display both original\n",
    "imshow(red_cat_orp[:, 24*128 : 27*128], title='Reduced Concatenated Ordered Residual Post')\n",
    "imshow(cat_orp[:, 24*128 : 27*128], title='Concatenated Ordered Residual Post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One is very reduced, but they're basically the same. To see if they're both doing the same job, we can plot the probability distributions you get from the output's logits. if the distributions are the same, we'll know that the reduction doesn't lose significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked version of original and reduced ORP\n",
    "red_stack_orp = einops.rearrange(red_cat_orp, 'num (sum d_model) -> (sum num) d_model', sum=p, num=p)[::p] # Each 113 block has the same output\n",
    "stack_orp = einops.rearrange(ordered_resid_post, 'sum num d_out -> (sum num) d_out')[::p] # Idem\n",
    "\n",
    "# Pass them through W_U to get logits\n",
    "logits = torch.from_numpy(stack_orp) @ params['unembed.W_U']\n",
    "red_logits = torch.from_numpy(red_stack_orp).float() @ params['unembed.W_U']\n",
    "\n",
    "# Take the softmax to get probability distribution\n",
    "probs = logits.softmax(dim=-1)\n",
    "red_probs = red_logits.softmax(dim=-1)\n",
    "\n",
    "# Display both to compare probability distributions\n",
    "imshow(probs, title=\"Original Probability Distribution\")\n",
    "imshow(red_probs, title=\"Reduced Probability Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES YESSSSSSSSSSSSSSSSSSSSSSSSS THIS IS ALL THAT FUCKING MATTERS YEAAAAAAAAAAAAAAAAAAH FUCK EVERYTHING FUCK EVERYONE ONLY ONE FREQUENCY FUCKING MATTERS!!!!\n",
    "\n",
    "**RESULT**: Yes, onlt that frequency matters. It impressed me how it's not even being rounded, all of the probabilities are 1 (or .9999 lol). This means that we only need to learn how to get from **resid_mid** to the reduced **resid_post**. Hopefully inspecting the MLP will tell us how it's done, so that we can reverse-engineer the process and find out what the vectors in **resid_mid** are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing 'resid_mid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've found out that the MLP takes the **resid_mid** and returns an array where inputs with an equivalent sum have very similar vectors. However, things are not so clear in the residual stream right after the attention later (and before the MLP), so our job is to figure out how to sort the vectors in **resid_mid** into their respective usms/classes without the MLP.\n",
    "\n",
    "If we manage to get a good understanding of how the different classes can be sorted (based on FFT, SVD), we'll be able to combine this with our knowledge of the self-attention layer to finish the interpretation of the model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've just realized that 'sorted' is better than 'ordered'\n",
    "sorted_resid_mid = ordered_resid_mid # It's a np array\n",
    "print(sorted_resid_mid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me first check if the sorting algorithm I wrote is working. To do that I'll use the same algorithm on the output, which will make it very evident whether I made a mistake or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to check if the sorting algorithm is working correctly by using it on the output\n",
    "\n",
    "# Get output probabilities for dataset\n",
    "unsorted_out = model(dataset)[:, -1, :].softmax(dim=-1) # Only prediction for '=' matters\n",
    "\n",
    "# Store all vectors in equivalence classes (same class if same sum)\n",
    "sorted_logits = [[] for _ in range(p)]\n",
    "\n",
    "for a in range(p):\n",
    "    for b in range(p):\n",
    "        sum = (a + b) % p\n",
    "        index = a * p + b\n",
    "        # Put vector of sum 'sum' in index class 'sum'\n",
    "        sorted_logits[sum].append(utils.to_numpy(unsorted_out[index]))\n",
    "\n",
    "# Make it into an ndarray so we can use numpy funcionalities\n",
    "sorted_logits = np.array(sorted_logits) # Has shape (p, p, p) -> (class, n, prob)\n",
    "\n",
    "# We expect an ordered list from 0 to 112\n",
    "print(sorted_logits[:, 0].argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SVD of each class of resid_mid\n",
    "SVD_rm = np.linalg.svd(sorted_resid_mid) # Returns tuple (U, S, V) will all of them\n",
    "\n",
    "# Get individual components\n",
    "U_rm = SVD_rm[0] # (113, 113, 113)\n",
    "S_rm = SVD_rm[1] # (113, 113)\n",
    "V_rm = SVD_rm[2] # (113, 128, 128)\n",
    "\n",
    "# Display maximum and mean 'importance' of ith index of S\n",
    "df = pd.DataFrame({'max': S_rm.max(axis=0), 'min': S_rm.min(axis=0), 'mean': S_rm.mean(axis=0)})\n",
    "px.line(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 graphs are very similar, which implies that the data is somewhat similar too. This is very good news! It means that each sum/class has a similar representation, which makes things more interpretable.\n",
    "\n",
    "We can see a very sharp drop from 4 to 5, so it seems like a good threshold. If we cannot get what we want from just the first 5 columns we'll use more. (no need, the first 5 carry basically all info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most significant columns upto up_to\n",
    "up_to = 5\n",
    "# Pick from 'n' different classes\n",
    "n_classes = 8\n",
    "\n",
    "indexes = np.random.randint(size=n_classes, low=0, high=p)\n",
    "\n",
    "fig_SVD_rm = make_subplots(\n",
    "    rows=3, cols=n_classes,\n",
    "    horizontal_spacing=0.02,\n",
    "    vertical_spacing=0.05,\n",
    "    row_titles=['U', 'U_fft', 'V'],\n",
    "    column_titles=[str(i) for i in indexes]\n",
    ")\n",
    "\n",
    "for col in range(n_classes):\n",
    "    # Pick a random class\n",
    "    n = indexes[col]\n",
    "\n",
    "    # Show first 5 rows of U * S\n",
    "    fig_SVD_rm.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=U_rm[n][:, :up_to] * S_rm[n][:up_to],\n",
    "            colorscale='RdBu',\n",
    "            zmid=0.,\n",
    "            showscale=False,\n",
    "        ), row=1, col=col+1\n",
    "    )\n",
    "    # Show first 5 rows of abs FFT of U * S\n",
    "    fig_SVD_rm.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=np.abs(np.fft.fft((U_rm[n][:, :up_to] * S_rm[n][:up_to]), axis=0)),\n",
    "            colorscale='RdBu',\n",
    "            zmid=0.,\n",
    "            showscale=False,\n",
    "        ), row=2, col=col+1\n",
    "    )\n",
    "    # Show first 5 rows of S * V\n",
    "    fig_SVD_rm.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=V_rm[n][:, :up_to] * S_rm[n][:up_to],\n",
    "            colorscale='RdBu',\n",
    "            zmid=0.,\n",
    "            showscale=False\n",
    "        ), row=3, col=col+1\n",
    "    )\n",
    "fig_SVD_rm.update_layout(height=800)\n",
    "fig_SVD_rm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVATION**: They all use (practically) **the same** frequency. This is great news!!! Or is it? I already knew, from the previous analysis, that they all shared similar frequencies. Nonetheless, this does give me some new information. \n",
    "\n",
    "**IDEA**: Reduce the FFT to only the positions that repeatedly appear, make a reduced version of the arrays and pass them all through the MLP. If it results in correct probabilities we may have figured it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_mlp(resid_mid):\n",
    "    '''\n",
    "    Pass batch through MLP and W_U, used to go from resid_mid to logits\n",
    "    '''\n",
    "    # turn resid_mid into tensor\n",
    "    resid_mid = torch.tensor(resid_mid, dtype=torch.float32)\n",
    "    # Pass through first linear layer\n",
    "    mlp_in = resid_mid @ params['blocks.0.mlp.W_in'] + params['blocks.0.mlp.b_in']\n",
    "    # Pass through ReLU\n",
    "    mlp_relu = torch.relu(mlp_in)\n",
    "    # Pass through second linear layer\n",
    "    mlp_out = mlp_relu @ params['blocks.0.mlp.W_out'] + params['blocks.0.mlp.b_out']\n",
    "    # Add to residual stream\n",
    "    resid_out =  resid_mid + mlp_out\n",
    "    # Return logits\n",
    "    logits = resid_out @ params['unembed.W_U'] + params['unembed.b_U']\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_fft(U):\n",
    "    '''\n",
    "    Receives U (batch, 113, 113) and returns a reduced version of U with only first 5 cols nonzero and simplified\n",
    "    Take the FFT and make all frequencies but one on each col zero (indexes below)\n",
    "    '''\n",
    "    U_fft = np.fft.fft(U, axis=1)\n",
    "\n",
    "    # Initialize array that will be reduced FFT, copy DC\n",
    "    U_fft_red = np.zeros_like(U_fft)\n",
    "    U_fft_red[:, 0, :5] = U_fft[:, 0, :5]\n",
    "\n",
    "    # Define which frequencies will be copied from each col\n",
    "    freq_index = [41, 8, 8, 19, 54]\n",
    "\n",
    "    for col in range(5):\n",
    "        # Copy main frequency into respective col\n",
    "        U_fft_red[:, freq_index[col], col] = U_fft[:, freq_index[col], col]\n",
    "        # Also copy the mirrored ones\n",
    "        U_fft_red[:, 113 - freq_index[col], col] = U_fft[:, 113 - freq_index[col], col]\n",
    "\n",
    "    # Take the inverse FFT\n",
    "    U_red = np.real(np.fft.ifft(U_fft_red, axis=1))\n",
    "    return U_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reduced U_rm\n",
    "U_rm_red = reduce_fft(U_rm)\n",
    "\n",
    "# Transform S_rm into diagonal matrix of shape (113, 113, 128\n",
    "S_rm_diag = [np.hstack([np.diag(S_rm[i]), np.zeros((113, 128-113))]) for i in range(p)]\n",
    "S_rm_diag = np.array(S_rm_diag)\n",
    "\n",
    "# Get reduced sorted_resid_mid\n",
    "red_srm = U_rm_red @ S_rm_diag @ V_rm # (113, 113, 128)\n",
    "\n",
    "# Stack red_srm for better visualization\n",
    "stack_red_srm = einops.rearrange(red_srm, 'sum num d_model -> (sum num) d_model')\n",
    "\n",
    "# Pass through MLP and get logits\n",
    "imshow(pass_mlp(stack_red_srm[::p]).softmax(dim=-1)) # I'm scared of large plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_mlp(red_srm).softmax(dim=-1).mean(axis=-2).max(axis=-1)[0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NICE***\n",
    "\n",
    "If you zoom-in you can see that almost all inputs return the correct probabilities. Only a few thin strips give the wrong prediction. This means that our dimensionality-reduction **WORKED!**, and interpreting the model will become much easier. Now we're really close to writing down the actual frequencies, I can't believe we're doing so well **FUCK YEAH SCIENCE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've analyzed 'resid_mid', but I wonder what info we can get by analyzing the encoding of '=' and what gets added to it by each head\n",
    "\n",
    "**IDEA**: Since the encoding of '=' is the same for all inputs, vectors coming from the attention layer (before being added to the encoding of '=') and resid_mid should have all the same properties other than their means. We can compare red_resid_mid with resid_mid, both subtracted from the encoding of '=', to check if my hypothesis holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining/mentioning variables we'll use\n",
    "\n",
    "# Get original and reduced sorted_resid_mid (113, 113, 128) -> (group, num, d_model)\n",
    "orig_sorted_resid_mid = torch.tensor(sorted_resid_mid) # Easier to deal with tensor for now\n",
    "red_sorted_resid_mid = torch.tensor(red_srm) # Renaming because why not\n",
    "\n",
    "# Get vector of '=' on resid_pre (128,)\n",
    "resid_pre_equals = cache['resid_pre', 0][0, -1] # All are equal, so we can pick just the first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
